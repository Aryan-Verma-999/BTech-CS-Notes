# -*- coding: utf-8 -*-
"""Chaotic_GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C1-Y1V-XmzO3i7DmvMhB8cikkNBNst_f
"""

import torch
from torch import nn
import numpy as np
import math
import matplotlib.pyplot as plt
from pylab import *
import tensorflow as tf
import torchvision
from torchvision import transforms
import os
import pandas as pd

def HenonMap(a,b,x,y):
	return y + 1.0 - a *x*x, b * x

def henon():
    # Simulation parameters
    # Control parameters:
    a = 1.4
    b = 0.3
    # The number of iterations to throw away
    nTransients = 100
    # The number of iterations to generate
    nIterates = 20000

    # Initial condition
    xtemp = 0.1
    ytemp = 0.3
    for n in range(0, nTransients):
        xtemp, ytemp = HenonMap(a, b, xtemp, ytemp)
    # Set up arrays of iterates (x_n,y_n) and set the initital condition
    x = [xtemp]
    y = [ytemp]
    # The main loop that generates iterates and stores them
    for n in range(0, nIterates):
        # at each iteration calculate (x_n+1,y_n+1)
        xtemp, ytemp = HenonMap(a, b, x[n], y[n])
        # and append to lists x and y
        x.append(xtemp)
        y.append(ytemp)
    return x,y

#coupled logistic map

def CoupledLogisticMaps(r,c,x1,y1):
	return r * x1 * (1.0 - x1) + c * y1, r * y1 * (1.0 - y1) + c * x1
def couple():
    # Control parameters:
    r = 3.5
    c = 0.05
    # The number of iterations to throw away
    nTransients1 = 100
    # The number of iterations to generate
    nIterates1 = 20000

    # Initial condition
    xtemp1 = 0.2
    ytemp1 = 0.3
    for n in range(0,nTransients1):
      xtemp1, ytemp1 = CoupledLogisticMaps(r,c,xtemp1,ytemp1)
    # Set up arrays of iterates (x1_n,y1_n) and set the initital condition
    x1 = [xtemp1]
    y1 = [ytemp1]
    # The main loop that generates iterates and stores them
    for n in range(0,nIterates1):
      # at each iteration calculate (x1_n+1,y1_n+1)
      xtemp1, ytemp1 = CoupledLogisticMaps(r,c,x1[n],y1[n])
      # and append to lists x1 and y1
      x1.append( xtemp1 )
      y1.append( ytemp1 )
    return x1,y1

xh,yh = henon()

xc,yc = couple()

#concatenateing xh,xc and yh, yc
xh = xh + xc
yh = yh + yc

torch.manual_seed(111)

train_data_length = 40000
train_data = torch.zeros((train_data_length, 2))

values = list(zip(xh, yh))
arr = np.array(values,float)
train_data = tf.convert_to_tensor(arr)

train_labels = torch.zeros(train_data_length)
train_set = [
    (train_data[i], train_labels[i]) for i in range(train_data_length)
]

train_set_np = [x[0].numpy().astype('float32') for x in train_set]

batch_size = 32
# train_loader = torch.utils.data.DataLoader(
#     train_set, batch_size=batch_size, shuffle=True
# )
train_loader = torch.utils.data.DataLoader(
    train_set_np, batch_size=batch_size, shuffle=True
)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(2, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1),
            nn.Sigmoid(),
        )

    def forward(self, x):
        output = self.model(x)
        return output

discriminator = Discriminator()

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(2, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, 2),
        )

    def forward(self, x):
        output = self.model(x)
        return output

generator = Generator()
IS = 256
lr = 0.001
num_epochs = 35
loss_function = nn.BCELoss()

optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)
optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)

for epoch in range(num_epochs):
    for n, real_samples in enumerate(train_loader):   ## for n, (real_samples, _) in enumerate(train_loader):
        # Data for training the discriminator
        real_samples_labels = torch.ones((batch_size, 1))
        latent_space_samples = torch.randn((batch_size, 2))
        generated_samples = generator(latent_space_samples)
        generated_samples_labels = torch.zeros((batch_size, 1))
        all_samples = torch.cat((real_samples, generated_samples))
        all_samples_labels = torch.cat(
            (real_samples_labels, generated_samples_labels)
        )
        # Training the discriminator
        discriminator.zero_grad()
        output_discriminator = discriminator(all_samples)
        loss_discriminator = loss_function(
           output_discriminator, all_samples_labels)
        loss_discriminator.backward()
        optimizer_discriminator.step()

        # Data for training the generator
        latent_space_samples = torch.randn((batch_size, 2))

        # Training the generator
        generator.zero_grad()
        generated_samples = generator(latent_space_samples)
        output_discriminator_generated = discriminator(generated_samples)
        loss_generator = loss_function(
              output_discriminator_generated, real_samples_labels
        )
        loss_generator.backward()
        optimizer_generator.step()
        # Show loss

        if epoch % 10 == 0 and n == batch_size - 1:
           print(f"Epoch: {epoch} Loss D.: {loss_discriminator}")
           print(f"Epoch: {epoch} Loss G.: {loss_generator}")

t = IS * IS
latent_space_samples = torch.randn(t, 2)
generated_samples = generator(latent_space_samples)
generated_samples = generated_samples.detach()
# print(generated_samples)
key = []
# print(len(generated_samples))
for i in range(t):
  gs = generated_samples[i][1]
  mulp = pow(10,7)
  igs = int(gs*mulp)
  igs = igs % 256
  key.append(igs)
# print(key)
# plt.plot(generated_samples[:, 0], generated_samples[:, 1], ".")
# plt.show()

key_df = pd.DataFrame(key)
key_df.to_csv("Key_file.csv")